# Финальный проект блока MLOPS курса hardml от karpov.courses

Вариант не идеальный, много аспектов не учтены в связи с отсутствией большого количества времени на детальную проработку решения.

## Из чего состоит

Всего я выделил 3 основных сервиса:

 - **serving** - энкодит предложение в эмбендинги через USE модель. Образ кастомный, Tensorflow/serving не взлетел, использовал sentence-transformers
 - **api** - получает запрос, отправляет в **serving**, после по эмбендингу определяет кластер, к которому запрос ближе всего и кидает запрос на нужный кластерный сервис
 - **cluster** - 4 сервиса (0 - 3, по количеству кластеров),  которые получив запрос, ищут ближайшего соседа из имеющихся предложений (используется FAISS) и возвращают самый ближайший match для запроса.

Построение FAISS описано в *logic_scripts/gens_preparation.py*

## Деплой

В данном проект есть допущение, что Docker registry уже сформирован на кластере.

Для подготовки всех образов вызывается скрипт *build_imgs.sh*, который создает нужные образы и пушит их в Registry

После подготовки образов необходимо запустить *deploy_service.sh* - используя Docker Stack он запустит необходимые сервисы, исходя из настроек в docker-compose.yml (какой сервис на какой ноде, какие порты и т.д.)

## Пример обращения
```
curl -d '{"instances":["Why so serious?"]}' -X POST  http://65.108.82.162:5050/process
```
```
{"match":"What is the best advice for aspiring entrepreneurs?"}
```
## Data transfer and generation update

Распределение файлов по нодам осуществляется через ***rsync***, применяется скрипт *logic_scripts/data_transfer.sh*.

Для обновления используется Rolling update c 20 секундным downtime, а так же Rollback, на случай если при обновлении сервиса что-то пойдет не так. Скрипт - *logic_scripts/gen_update.sh*

## Контрольные вопросы
*1. Точно ли вы подняли все, что нужно QA системе? Если вы закодили все внутри одного приложения, то это не зачтется :(*

**Да, подняты все необходимые сервисы, различными приложениями.**

*2. Каким образом файлы с индексами попадают на целевые машины? Попадают ли индексы адресно или происходит broadcast одного файла на все сервера?*

**Происходит broadcast одного файла, так как этот вариант мне показался легче в реализации**

*3. Написали ли ли вы стратегию обновления индекса в системе? Что это за стратегия: Blue / Green или Rolling? А есть ли у вашей стратегии downtime?*

**Да, написал. Стратегия - Rolling update. Downtime - 20 sec**

*4. При обновлении индекса обновляются ли необходимые параметры в других частях системы?
Т.е. не случится ли такого, что gateway направляет запрос не на тот индекс если во время обновления что то пошло не так? Не забыли ли вы про идемпотентность обновлений в вашей стратегии?*

**Не смог правильно обработать данный аспект. Обнаружил ситуацию, что количество центров кластеров больше, чем данным по кластерам. Данный вопрос решил через велосипед отправляя данные 5го кластера на 2ой (у них была самая большая косинусная близость)**

*5. Что будет, если индекс не сможет обновиться? (например, попался битый файл индекса)*

**Произойдет Rollback**

*6. Может, вы не поленились и предусмотрели ли какие нибудь failover?*

**Не смог придумать, как реализовать данный аспект**

*7. Вы использовали уже готовые инструменты, знакомые многим или решили написали свой shellсипед?*

**Использовал свой shellсипед**

*8. А что с самими приложениями сервинга? Нет перегружен ли его запуск внешними зависимостями, правильно ли выключается и поднимается приложение. Существует способ быстро проверить здорово ли приложение?*

**Как мне показалось перегрузки нет. Healthcheck и graceful shutdown не реализован в связи с малым количеством времени, затраченным на проработку архитектуры**


